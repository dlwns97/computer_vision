{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM17RchDR2sDE6jc94M0RVw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RWIw5xCILFEB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658216660919,"user_tz":-540,"elapsed":7419,"user":{"displayName":"조이준","userId":"18430695341039920489"}},"outputId":"159bedbe-7863-423a-e601-9679703c43a3"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","1/1 [==============================] - 2s 2s/step - loss: 1.6023 - accuracy: 0.3333\n","Epoch 2/50\n","1/1 [==============================] - 0s 15ms/step - loss: 1.5214 - accuracy: 0.3333\n","Epoch 3/50\n","1/1 [==============================] - 0s 14ms/step - loss: 1.4530 - accuracy: 0.3333\n","Epoch 4/50\n","1/1 [==============================] - 0s 12ms/step - loss: 1.3705 - accuracy: 0.3333\n","Epoch 5/50\n","1/1 [==============================] - 0s 11ms/step - loss: 1.2520 - accuracy: 0.3333\n","Epoch 6/50\n","1/1 [==============================] - 0s 13ms/step - loss: 1.1110 - accuracy: 0.3333\n","Epoch 7/50\n","1/1 [==============================] - 0s 14ms/step - loss: 0.9539 - accuracy: 0.5000\n","Epoch 8/50\n","1/1 [==============================] - 0s 14ms/step - loss: 0.8009 - accuracy: 1.0000\n","Epoch 9/50\n","1/1 [==============================] - 0s 12ms/step - loss: 0.6910 - accuracy: 1.0000\n","Epoch 10/50\n","1/1 [==============================] - 0s 14ms/step - loss: 0.5928 - accuracy: 1.0000\n","Epoch 11/50\n","1/1 [==============================] - 0s 13ms/step - loss: 0.5140 - accuracy: 1.0000\n","Epoch 12/50\n","1/1 [==============================] - 0s 11ms/step - loss: 0.4478 - accuracy: 1.0000\n","Epoch 13/50\n","1/1 [==============================] - 0s 12ms/step - loss: 0.3811 - accuracy: 1.0000\n","Epoch 14/50\n","1/1 [==============================] - 0s 13ms/step - loss: 0.3303 - accuracy: 1.0000\n","Epoch 15/50\n","1/1 [==============================] - 0s 12ms/step - loss: 0.2807 - accuracy: 1.0000\n","Epoch 16/50\n","1/1 [==============================] - 0s 12ms/step - loss: 0.2320 - accuracy: 1.0000\n","Epoch 17/50\n","1/1 [==============================] - 0s 20ms/step - loss: 0.1916 - accuracy: 1.0000\n","Epoch 18/50\n","1/1 [==============================] - 0s 13ms/step - loss: 0.1538 - accuracy: 1.0000\n","Epoch 19/50\n","1/1 [==============================] - 0s 14ms/step - loss: 0.1193 - accuracy: 1.0000\n","Epoch 20/50\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0926 - accuracy: 1.0000\n","Epoch 21/50\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0721 - accuracy: 1.0000\n","Epoch 22/50\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0552 - accuracy: 1.0000\n","Epoch 23/50\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0423 - accuracy: 1.0000\n","Epoch 24/50\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0335 - accuracy: 1.0000\n","Epoch 25/50\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0273 - accuracy: 1.0000\n","Epoch 26/50\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0222 - accuracy: 1.0000\n","Epoch 27/50\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0181 - accuracy: 1.0000\n","Epoch 28/50\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0151 - accuracy: 1.0000\n","Epoch 29/50\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0132 - accuracy: 1.0000\n","Epoch 30/50\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0117 - accuracy: 1.0000\n","Epoch 31/50\n","1/1 [==============================] - 0s 9ms/step - loss: 0.0104 - accuracy: 1.0000\n","Epoch 32/50\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0092 - accuracy: 1.0000\n","Epoch 33/50\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0081 - accuracy: 1.0000\n","Epoch 34/50\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0072 - accuracy: 1.0000\n","Epoch 35/50\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0066 - accuracy: 1.0000\n","Epoch 36/50\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0061 - accuracy: 1.0000\n","Epoch 37/50\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0057 - accuracy: 1.0000\n","Epoch 38/50\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0053 - accuracy: 1.0000\n","Epoch 39/50\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0049 - accuracy: 1.0000\n","Epoch 40/50\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0045 - accuracy: 1.0000\n","Epoch 41/50\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0042 - accuracy: 1.0000\n","Epoch 42/50\n","1/1 [==============================] - 0s 11ms/step - loss: 0.0040 - accuracy: 1.0000\n","Epoch 43/50\n","1/1 [==============================] - 0s 10ms/step - loss: 0.0038 - accuracy: 1.0000\n","Epoch 44/50\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0036 - accuracy: 1.0000\n","Epoch 45/50\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0035 - accuracy: 1.0000\n","Epoch 46/50\n","1/1 [==============================] - 0s 17ms/step - loss: 0.0033 - accuracy: 1.0000\n","Epoch 47/50\n","1/1 [==============================] - 0s 20ms/step - loss: 0.0031 - accuracy: 1.0000\n","Epoch 48/50\n","1/1 [==============================] - 0s 24ms/step - loss: 0.0030 - accuracy: 1.0000\n","Epoch 49/50\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0029 - accuracy: 1.0000\n","Epoch 50/50\n","1/1 [==============================] - 0s 12ms/step - loss: 0.0027 - accuracy: 1.0000\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," rnn (RNN)                   (None, 6, 5)              220       \n","                                                                 \n"," time_distributed (TimeDistr  (None, 6, 5)             30        \n"," ibuted)                                                         \n","                                                                 \n","=================================================================\n","Total params: 250\n","Trainable params: 250\n","Non-trainable params: 0\n","_________________________________________________________________\n","[[1.85126602e-03 9.97654140e-01 2.59259396e-04 2.51848396e-05\n","  2.10161175e-04]\n"," [9.98303652e-01 1.56183285e-03 1.03645216e-04 3.05272479e-05\n","  1.92798254e-07]\n"," [2.19660011e-04 2.77544954e-04 9.97120142e-01 2.22738506e-03\n","  1.55275222e-04]\n"," [1.37428842e-05 9.10127582e-08 1.92241056e-03 9.97933388e-01\n","  1.30408618e-04]\n"," [3.79300286e-06 9.29679345e-07 3.50297545e-04 9.96845245e-01\n","  2.79973564e-03]\n"," [2.64285749e-08 1.52725924e-03 1.44246087e-05 2.11934838e-03\n","  9.96338964e-01]]\n","\tPrediction str:  ihello\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","\n","idx2char = ['h','i','e','l','o']\n","# Teach hello: hihell, ihello\n","y_data = [[1,0,2,3,3,4]]\n","\n","# hyperparameter\n","num_classes = 5\n","input_dim = 5 # 가장 안쪽에 인자의 개수\n","sequence_length = 6 # 단어의 개수 또는 단어의 길이 등\n","learning_rate = 0.1\n","\n","# hihell\n","# (batch size, sequence_length, input_dim)\n","x_one_hot = np.array([[[1,0,0,0,0],\n","                       [0,1,0,0,0],\n","                       [1,0,0,0,0],\n","                       [0,0,1,0,0],\n","                       [0,0,0,1,0],\n","                       [0,0,0,1,0]]],\n","                     dtype=np.float32)\n","y_one_hot = tf.keras.utils.to_categorical(y_data, num_classes=num_classes)\n","\n","tf.model = tf.keras.Sequential()\n","\n","# make cell and add it to RNN layer\n","cell = tf.keras.layers.LSTMCell(units=num_classes, input_shape=(sequence_length,input_dim))\n","tf.model.add(tf.keras.layers.RNN(cell=cell, return_sequences=True))\n","'''\n","return_sequences=True를 설정하면 RNN 레이어가 \n","각 샘플(샘플 및 타임스텝당 하나의 벡터)에 대한 전체 출력 시퀀스도 반환할 수 있습니다\n","'''\n","tf.keras.layers.RNN()\n","\n","# single LSTM layer can be used as well instead of creating LSTMCell\n","\n","# add Fully connected layer\n","tf.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=num_classes, activation='softmax')))\n","tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n","# train\n","tf.model.fit(x_one_hot, y_one_hot, epochs=50)\n","tf.model.summary()\n","\n","predictions = tf.model.predict(x_one_hot)\n","for i, prediction in enumerate(predictions):\n","  print(prediction)\n","  result_str = [idx2char[c] for c in np.argmax(prediction, axis=1)]\n","  print(\"\\tPrediction str: \", ''.join(result_str))"]},{"cell_type":"code","source":["sample = \" if you want you\"\n","idx2char = list(set(sample))\n","char2idx = {c: i for i, c in enumerate(idx2char)}\n","\n","# hyper parameters\n","learning_rate = 0.1\n","dic_size = len(char2idx) # RNN input\n","hidden_size = len(char2idx) # RNN output\n","num_classes = len(char2idx) # final output, usually softmax\n","batch_size = 1\n","sequence_length = len(sample) - 1\n","\n","sample_idx = [char2idx[c] for c in sample]\n","x_data = [sample_idx[:-1]] # hello에서 hell이면 [0,1,2,2]\n","y_data = [sample_idx[1:]] # [1,2,2,3]\n","\n","x_one_hot_eager = tf.one_hot(x_data, num_classes)\n","x_one_hot_numpy = tf.keras.utils.to_categorical(x_data, num_classes=num_classes)\n","y_one_hot_eager = tf.one_hot(y_data, num_classes)\n","#y_one_hot_numpy = tf.keras.utils.to_categorical(y_data, num_classes=num_classes)\n","\n","tf.model = tf.keras.Sequential()\n","tf.model.add(tf.keras.layers.LSTM(units=num_classes, input_shape=(sequence_length,x_one_hot_eager.shape[2]),return_sequences=True))\n","tf.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=num_classes, activation='softmax')))\n","tf.model.summary()\n","tf.model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n","\n","# training\n","tf.model.fit(x_one_hot_eager,y_one_hot_eager, epochs=50)\n","\n","# prediction\n","predictions = tf.model.predict(x_one_hot_eager)\n","\n","for i, prediction in enumerate(predictions):\n","  result_str = [idx2char[c] for c in np.argmax(prediction, axis=1)]\n","  print(\"\\tPrediction str :\", ''.join(result_str))\n"],"metadata":{"id":"zeCCXL_7m_-J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658217628481,"user_tz":-540,"elapsed":4409,"user":{"displayName":"조이준","userId":"18430695341039920489"}},"outputId":"b170df91-a6a0-4f99-9e14-f331d05bb062"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm (LSTM)                 (None, 15, 10)            840       \n","                                                                 \n"," time_distributed_1 (TimeDis  (None, 15, 10)           110       \n"," tributed)                                                       \n","                                                                 \n","=================================================================\n","Total params: 950\n","Trainable params: 950\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 2s 2s/step - loss: 2.2889 - accuracy: 0.1333\n","Epoch 2/50\n","1/1 [==============================] - 0s 12ms/step - loss: 2.1489 - accuracy: 0.2000\n","Epoch 3/50\n","1/1 [==============================] - 0s 17ms/step - loss: 2.0084 - accuracy: 0.2000\n","Epoch 4/50\n","1/1 [==============================] - 0s 15ms/step - loss: 1.8513 - accuracy: 0.3333\n","Epoch 5/50\n","1/1 [==============================] - 0s 16ms/step - loss: 1.6468 - accuracy: 0.5333\n","Epoch 6/50\n","1/1 [==============================] - 0s 17ms/step - loss: 1.4038 - accuracy: 0.6000\n","Epoch 7/50\n","1/1 [==============================] - 0s 21ms/step - loss: 1.1403 - accuracy: 0.8000\n","Epoch 8/50\n","1/1 [==============================] - 0s 20ms/step - loss: 0.8892 - accuracy: 0.8667\n","Epoch 9/50\n","1/1 [==============================] - 0s 15ms/step - loss: 0.6544 - accuracy: 0.9333\n","Epoch 10/50\n","1/1 [==============================] - 0s 14ms/step - loss: 0.4596 - accuracy: 1.0000\n","Epoch 11/50\n","1/1 [==============================] - 0s 18ms/step - loss: 0.3184 - accuracy: 1.0000\n","Epoch 12/50\n","1/1 [==============================] - 0s 13ms/step - loss: 0.2195 - accuracy: 1.0000\n","Epoch 13/50\n","1/1 [==============================] - 0s 16ms/step - loss: 0.1487 - accuracy: 1.0000\n","Epoch 14/50\n","1/1 [==============================] - 0s 16ms/step - loss: 0.1000 - accuracy: 1.0000\n","Epoch 15/50\n","1/1 [==============================] - 0s 19ms/step - loss: 0.0670 - accuracy: 1.0000\n","Epoch 16/50\n","1/1 [==============================] - 0s 18ms/step - loss: 0.0451 - accuracy: 1.0000\n","Epoch 17/50\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0308 - accuracy: 1.0000\n","Epoch 18/50\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0216 - accuracy: 1.0000\n","Epoch 19/50\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0155 - accuracy: 1.0000\n","Epoch 20/50\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0115 - accuracy: 1.0000\n","Epoch 21/50\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0087 - accuracy: 1.0000\n","Epoch 22/50\n","1/1 [==============================] - 0s 21ms/step - loss: 0.0068 - accuracy: 1.0000\n","Epoch 23/50\n","1/1 [==============================] - 0s 21ms/step - loss: 0.0054 - accuracy: 1.0000\n","Epoch 24/50\n","1/1 [==============================] - 0s 14ms/step - loss: 0.0044 - accuracy: 1.0000\n","Epoch 25/50\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0036 - accuracy: 1.0000\n","Epoch 26/50\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0030 - accuracy: 1.0000\n","Epoch 27/50\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0026 - accuracy: 1.0000\n","Epoch 28/50\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0022 - accuracy: 1.0000\n","Epoch 29/50\n","1/1 [==============================] - 0s 19ms/step - loss: 0.0019 - accuracy: 1.0000\n","Epoch 30/50\n","1/1 [==============================] - 0s 18ms/step - loss: 0.0017 - accuracy: 1.0000\n","Epoch 31/50\n","1/1 [==============================] - 0s 13ms/step - loss: 0.0015 - accuracy: 1.0000\n","Epoch 32/50\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0014 - accuracy: 1.0000\n","Epoch 33/50\n","1/1 [==============================] - 0s 23ms/step - loss: 0.0012 - accuracy: 1.0000\n","Epoch 34/50\n","1/1 [==============================] - 0s 15ms/step - loss: 0.0011 - accuracy: 1.0000\n","Epoch 35/50\n","1/1 [==============================] - 0s 16ms/step - loss: 0.0010 - accuracy: 1.0000\n","Epoch 36/50\n","1/1 [==============================] - 0s 15ms/step - loss: 9.4009e-04 - accuracy: 1.0000\n","Epoch 37/50\n","1/1 [==============================] - 0s 14ms/step - loss: 8.7207e-04 - accuracy: 1.0000\n","Epoch 38/50\n","1/1 [==============================] - 0s 23ms/step - loss: 8.1335e-04 - accuracy: 1.0000\n","Epoch 39/50\n","1/1 [==============================] - 0s 21ms/step - loss: 7.6235e-04 - accuracy: 1.0000\n","Epoch 40/50\n","1/1 [==============================] - 0s 17ms/step - loss: 7.1774e-04 - accuracy: 1.0000\n","Epoch 41/50\n","1/1 [==============================] - 0s 15ms/step - loss: 6.7858e-04 - accuracy: 1.0000\n","Epoch 42/50\n","1/1 [==============================] - 0s 22ms/step - loss: 6.4404e-04 - accuracy: 1.0000\n","Epoch 43/50\n","1/1 [==============================] - 0s 13ms/step - loss: 6.1337e-04 - accuracy: 1.0000\n","Epoch 44/50\n","1/1 [==============================] - 0s 20ms/step - loss: 5.8611e-04 - accuracy: 1.0000\n","Epoch 45/50\n","1/1 [==============================] - 0s 16ms/step - loss: 5.6172e-04 - accuracy: 1.0000\n","Epoch 46/50\n","1/1 [==============================] - 0s 17ms/step - loss: 5.3986e-04 - accuracy: 1.0000\n","Epoch 47/50\n","1/1 [==============================] - 0s 17ms/step - loss: 5.2018e-04 - accuracy: 1.0000\n","Epoch 48/50\n","1/1 [==============================] - 0s 16ms/step - loss: 5.0240e-04 - accuracy: 1.0000\n","Epoch 49/50\n","1/1 [==============================] - 0s 20ms/step - loss: 4.8629e-04 - accuracy: 1.0000\n","Epoch 50/50\n","1/1 [==============================] - 0s 18ms/step - loss: 4.7168e-04 - accuracy: 1.0000\n","\tPrediction str : if you want you\n"]}]},{"cell_type":"code","source":["# 훨씬 더 긴 문장을 가지고\n","# 위의 RNN으로 진행하면 학습이 잘 진행되지 않음\n","# RNN이 너무 얕고 좁기 때문\n","# 더 깊고 넓은 RNN으로 구성해야 함\n","\n","sentence = (\"if you want to build a ship, don't drum up people together to \"\n","            \"collect wood and don't assign them tasks and work, but rather \"\n","            \"teach them to long for the endless immensity of the sea.\")\n","char_set = list(set(sentence))\n","char_dic = {w: i for i, w in enumerate(char_set)}\n","\n","# hyper parameter\n","learning_rate = 0.1\n","data_dim = len(char_set)\n","hidden_size = len(char_set)\n","num_classes = len(char_set)\n","sequence_length = 10 # Arbitrary number\n","\n","dataX = []\n","dataY = []\n","for i in range(0, len(sentence)-sequence_length):\n","  x_str = sentence[i:i+sequence_length]\n","  y_str = sentence[i+1:i+sequence_length+1]\n","\n","  x = [char_dic[c] for c in x_str]\n","  y = [char_dic[c] for c in y_str]\n","\n","  dataX.append(x)\n","  dataY.append(y)\n","batch_size = len(dataX)\n","\n","# One-hot encoding\n","x_one_hot = tf.one_hot(dataX, num_classes)\n","y_one_hot = tf.one_hot(dataY, num_classes)\n","\n","tf.model = tf.keras.Sequential()\n","tf.model.add(tf.keras.layers.LSTM(units=num_classes, input_shape=(sequence_length, x_one_hot.shape[2]), return_sequences=True))\n","tf.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=num_classes, activation='softmax')))\n","tf.model.summary()\n","tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n","\n","# training\n","tf.model.fit(x_one_hot, y_one_hot, epochs=100)\n","\n","results = tf.model.predict(x_one_hot)\n","\n","for i, result in enumerate(results):\n","  index = np.argmax(result, axis=1)\n","  if i==0:\n","    print(''.join([char_set[c] for c in index]), end=' ')\n","  else:\n","    print(char_set[index[-1]], end='')\n"],"metadata":{"id":"ht5nzFuTsbwb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658217747802,"user_tz":-540,"elapsed":10016,"user":{"displayName":"조이준","userId":"18430695341039920489"}},"outputId":"0563dcf5-4c27-4acd-ba84-f566191f2f9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lstm_1 (LSTM)               (None, 10, 25)            5100      \n","                                                                 \n"," time_distributed_2 (TimeDis  (None, 10, 25)           650       \n"," tributed)                                                       \n","                                                                 \n","=================================================================\n","Total params: 5,750\n","Trainable params: 5,750\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["6/6 [==============================] - 2s 11ms/step - loss: 2.9387 - accuracy: 0.1888\n","Epoch 2/100\n","6/6 [==============================] - 0s 8ms/step - loss: 2.1789 - accuracy: 0.3829\n","Epoch 3/100\n","6/6 [==============================] - 0s 11ms/step - loss: 1.5085 - accuracy: 0.5153\n","Epoch 4/100\n","6/6 [==============================] - 0s 8ms/step - loss: 1.0163 - accuracy: 0.6871\n","Epoch 5/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.6678 - accuracy: 0.8141\n","Epoch 6/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.5096 - accuracy: 0.8512\n","Epoch 7/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.4210 - accuracy: 0.8629\n","Epoch 8/100\n","6/6 [==============================] - 0s 10ms/step - loss: 0.3718 - accuracy: 0.8741\n","Epoch 9/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.3490 - accuracy: 0.8806\n","Epoch 10/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.3157 - accuracy: 0.8824\n","Epoch 11/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.3146 - accuracy: 0.8788\n","Epoch 12/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.3026 - accuracy: 0.8818\n","Epoch 13/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2952 - accuracy: 0.8859\n","Epoch 14/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2941 - accuracy: 0.8806\n","Epoch 15/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2950 - accuracy: 0.8894\n","Epoch 16/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2994 - accuracy: 0.8812\n","Epoch 17/100\n","6/6 [==============================] - 0s 10ms/step - loss: 0.3166 - accuracy: 0.8729\n","Epoch 18/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.3144 - accuracy: 0.8794\n","Epoch 19/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.3153 - accuracy: 0.8765\n","Epoch 20/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2945 - accuracy: 0.8853\n","Epoch 21/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2804 - accuracy: 0.8894\n","Epoch 22/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2973 - accuracy: 0.8800\n","Epoch 23/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.3015 - accuracy: 0.8765\n","Epoch 24/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.3047 - accuracy: 0.8800\n","Epoch 25/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.3030 - accuracy: 0.8782\n","Epoch 26/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2997 - accuracy: 0.8824\n","Epoch 27/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2888 - accuracy: 0.8894\n","Epoch 28/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2826 - accuracy: 0.8882\n","Epoch 29/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2680 - accuracy: 0.8871\n","Epoch 30/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2713 - accuracy: 0.8847\n","Epoch 31/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2607 - accuracy: 0.8929\n","Epoch 32/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2720 - accuracy: 0.8900\n","Epoch 33/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2655 - accuracy: 0.8941\n","Epoch 34/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2661 - accuracy: 0.8888\n","Epoch 35/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2675 - accuracy: 0.8918\n","Epoch 36/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2682 - accuracy: 0.8912\n","Epoch 37/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2610 - accuracy: 0.8912\n","Epoch 38/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2546 - accuracy: 0.8876\n","Epoch 39/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2600 - accuracy: 0.8918\n","Epoch 40/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2575 - accuracy: 0.8888\n","Epoch 41/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2616 - accuracy: 0.8876\n","Epoch 42/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2595 - accuracy: 0.8876\n","Epoch 43/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2589 - accuracy: 0.8918\n","Epoch 44/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2559 - accuracy: 0.8888\n","Epoch 45/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2557 - accuracy: 0.8906\n","Epoch 46/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2535 - accuracy: 0.8847\n","Epoch 47/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2562 - accuracy: 0.8882\n","Epoch 48/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2564 - accuracy: 0.8906\n","Epoch 49/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2570 - accuracy: 0.8876\n","Epoch 50/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2532 - accuracy: 0.8865\n","Epoch 51/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2579 - accuracy: 0.8871\n","Epoch 52/100\n","6/6 [==============================] - 0s 7ms/step - loss: 0.2548 - accuracy: 0.8888\n","Epoch 53/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2567 - accuracy: 0.8882\n","Epoch 54/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2525 - accuracy: 0.8882\n","Epoch 55/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2514 - accuracy: 0.8894\n","Epoch 56/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2533 - accuracy: 0.8900\n","Epoch 57/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2556 - accuracy: 0.8888\n","Epoch 58/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2528 - accuracy: 0.8924\n","Epoch 59/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2526 - accuracy: 0.8894\n","Epoch 60/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2537 - accuracy: 0.8853\n","Epoch 61/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2538 - accuracy: 0.8829\n","Epoch 62/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2531 - accuracy: 0.8906\n","Epoch 63/100\n","6/6 [==============================] - 0s 11ms/step - loss: 0.2503 - accuracy: 0.8918\n","Epoch 64/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2527 - accuracy: 0.8912\n","Epoch 65/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2512 - accuracy: 0.8924\n","Epoch 66/100\n","6/6 [==============================] - 0s 10ms/step - loss: 0.2557 - accuracy: 0.8841\n","Epoch 67/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2595 - accuracy: 0.8876\n","Epoch 68/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2556 - accuracy: 0.8906\n","Epoch 69/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2536 - accuracy: 0.8929\n","Epoch 70/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2567 - accuracy: 0.8882\n","Epoch 71/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2584 - accuracy: 0.8859\n","Epoch 72/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2550 - accuracy: 0.8882\n","Epoch 73/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2517 - accuracy: 0.8912\n","Epoch 74/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2563 - accuracy: 0.8912\n","Epoch 75/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2575 - accuracy: 0.8888\n","Epoch 76/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2532 - accuracy: 0.8918\n","Epoch 77/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2574 - accuracy: 0.8906\n","Epoch 78/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2576 - accuracy: 0.8871\n","Epoch 79/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2555 - accuracy: 0.8865\n","Epoch 80/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2573 - accuracy: 0.8894\n","Epoch 81/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2571 - accuracy: 0.8894\n","Epoch 82/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2535 - accuracy: 0.8906\n","Epoch 83/100\n","6/6 [==============================] - 0s 12ms/step - loss: 0.2519 - accuracy: 0.8912\n","Epoch 84/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2513 - accuracy: 0.8888\n","Epoch 85/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2534 - accuracy: 0.8882\n","Epoch 86/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2480 - accuracy: 0.8900\n","Epoch 87/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2505 - accuracy: 0.8906\n","Epoch 88/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2533 - accuracy: 0.8894\n","Epoch 89/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2513 - accuracy: 0.8900\n","Epoch 90/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2563 - accuracy: 0.8882\n","Epoch 91/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2508 - accuracy: 0.8865\n","Epoch 92/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2533 - accuracy: 0.8912\n","Epoch 93/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2518 - accuracy: 0.8929\n","Epoch 94/100\n","6/6 [==============================] - 0s 10ms/step - loss: 0.2505 - accuracy: 0.8918\n","Epoch 95/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2505 - accuracy: 0.8894\n","Epoch 96/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2485 - accuracy: 0.8882\n","Epoch 97/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2482 - accuracy: 0.8906\n","Epoch 98/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2547 - accuracy: 0.8900\n","Epoch 99/100\n","6/6 [==============================] - 0s 8ms/step - loss: 0.2538 - accuracy: 0.8841\n","Epoch 100/100\n","6/6 [==============================] - 0s 9ms/step - loss: 0.2573 - accuracy: 0.8859\n","t you want  to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea."]}]},{"cell_type":"code","source":[""],"metadata":{"id":"bZsclqmZeqrK"},"execution_count":null,"outputs":[]}]}