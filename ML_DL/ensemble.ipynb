{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ensemble.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNX0WIjhcwfzKZ1mcF/cp7n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EIZoNTZz4hw0","executionInfo":{"status":"ok","timestamp":1640700581585,"user_tz":-540,"elapsed":1479,"user":{"displayName":"조이준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18430695341039920489"}},"outputId":"8da85252-120b-44ba-d740-43d5514cae9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["LogisticRegression  :  1.0\n","RandomForestClassifier  :  0.9333333333333333\n","SVC  :  1.0\n","VotingClassifier  :  1.0\n"]}],"source":["from sklearn.datasets import load_iris\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import train_test_split\n","\n","# 데이터셋 로드\n","iris = load_iris()\n","x = iris.data[:,2:] # 꽃잎의 길이와 너비\n","y = iris.target\n","\n","x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, \n","                                                    random_state=2021, shuffle=True)\n","# 약한 학습기 구축\n","log_model = LogisticRegression()\n","rnd_model = RandomForestClassifier()\n","svm_model = SVC()\n","\n","# 앙상블 모델 구축\n","voting_model = VotingClassifier(\n","    estimators = [('lr', log_model),\n","                  ('rf', rnd_model),\n","                  ('svc',svm_model)],\n","                  voting = 'hard' # 직접투표 방법\n",")\n","# 앙상블 모델 학습\n","voting_model.fit(x_train, y_train)\n","\n","# 모델 비교\n","for model in (log_model,rnd_model,svm_model,voting_model):\n","  model.fit(x_train,y_train)\n","  y_pred = model.predict(x_test)\n","  print(model.__class__.__name__,\" : \",accuracy_score(y_test,y_pred))"]},{"cell_type":"code","source":["# 사이킷 런에서 배깅과 페이스팅\n","\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# 모델 구축\n","bag_model = BaggingClassifier(\n","    DecisionTreeClassifier(), # 약한 학습기 (결정 트리)\n","    n_estimators=500, # 약한 학습기 (결정 트리) 500개 +> 첫 두 줄이 랜덤 포레스트 쓴다는 뜻\n","    max_samples=0.05, # 0~1 사이의 실수선택 (실수 * 샘플 수)\n","    bootstrap=True,\n","    n_jobs=-1 # +> 훈련에 사용할 CPU 코어의 수, -1 쓰면 컴퓨터의 모든 코어 사용\n",")\n","# 모델 학습\n","bag_model.fit(x_train, y_train)\n","\n","# 모델 예측\n","y_pred = bag_model.predict(x_test)\n","\n","# 모델 평가\n","print(bag_model.__class__.__name__, \" : \", accuracy_score(y_test,y_pred))"],"metadata":{"id":"cZZ46RGF8dMi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640700947233,"user_tz":-540,"elapsed":2223,"user":{"displayName":"조이준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18430695341039920489"}},"outputId":"0d013726-6c0a-4692-ca30-affb15cae89e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["BaggingClassifier  :  0.9777777777777777\n"]}]},{"cell_type":"code","source":["# random forest 방식\n","# 전체 특성 중에서 최선의 특성을 찾는 것이 아니라.\n","# 무작위로 선택한 특성들 중에서 최선의 특성을 찾는 방식을\n","# 채택하여 무작위성을 가지게 된다.\n","# 분산을 전체적으로 낮추어 더 훌륭한 모델을 만든다.\n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# 랜덤포레스트 모델 구축\n","rnd_model = RandomForestClassifier(\n","    n_estimators = 500,\n","    max_leaf_nodes = 16,\n","    n_jobs = -1\n",")\n","\n","# 모델 학습\n","rnd_model.fit(x_train, y_train)\n","\n","# 모델 예측\n","y_predrf = rnd_model.predict(x_test)\n","\n","# 모델 평가\n","print(\"rnd_model : \", accuracy_score(y_predrf, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-EujLwUX4G0","executionInfo":{"status":"ok","timestamp":1640702382306,"user_tz":-540,"elapsed":1866,"user":{"displayName":"조이준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18430695341039920489"}},"outputId":"0baa6eef-50d7-4a1c-d069-ea20dc7e3ebd"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["rnd_model :  0.9555555555555556\n"]}]},{"cell_type":"markdown","source":[""],"metadata":{"id":"US9FBUjcjcvA"}},{"cell_type":"code","source":["from sklearn.ensemble import AdaBoostClassifier # => 잘 학습하지 못하는 것에 대해서 가중치를 두겠다.\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# 아다부스트 모델 구축\n","# 아다부스트의 학습기 : Decision Tree (max depth = 1 ) 사용\n","# 학습기 개수 (n_estimators) : 200 개\n","# SAMME(Stagewise Additive Modeling using a Multiclass Exponential loss function) 알고리즘 사용\n","# 기본 학습기가 확률 추정(predict proba) 이 가능하면 SAMME.R 사용 => 일반적으로 성능이 더 좋음\n","ada_model = AdaBoostClassifier(\n","    DecisionTreeClassifier(max_depth=1),\n","    n_estimators=200,\n","    algorithm='SAMME.R',\n","    learning_rate=0.5\n",")\n","\n","# 모델 학습\n","ada_model.fit(x,y)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AY5PK_o3ey99","executionInfo":{"status":"ok","timestamp":1640703531961,"user_tz":-540,"elapsed":749,"user":{"displayName":"조이준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18430695341039920489"}},"outputId":"e971da20-983b-47b8-b335-644657e22c03"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n","                   learning_rate=0.5, n_estimators=200)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeRegressor\n","\n","# 결정트리 (max_depth3) 모델 구축 및 학습\n","tree_reg_model_1 = DecisionTreeRegressor(max_depth=3)\n","tree_reg_model_1.fit(x,y)\n","\n","# 첫 번째 학습기에서 발생한 잔차를 목적함수로 모델 학습\n","residual_1 = y - tree_reg_model_1.predict(x)\n","tree_reg_model_2 = DecisionTreeRegressor(max_depth=3)\n","tree_reg_model_2.fit(x, residual_1)\n","\n","# 두 번째 학습기에서 발생한 잔차를 목적함수로 모델 학습\n","residual_2 = y - tree_reg_model_2.predict(x)\n","tree_reg_model_3 = DecisionTreeRegressor(max_depth=3)\n","tree_reg_model_3.fit(x, residual_2)\n","\n","# 새로운 데이터를 세 개의 트리를 폼한한 앙상블 모델로 예측\n","x_new = [[1.4,0.2]]\n","prediction = sum(tree.predict(x_new) for tree in [tree_reg_model_1, tree_reg_model_2, tree_reg_model_3])\n","prediction"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QJ0He3XnlD6M","executionInfo":{"status":"ok","timestamp":1640704271621,"user_tz":-540,"elapsed":282,"user":{"displayName":"조이준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18430695341039920489"}},"outputId":"f404efa4-05db-45f8-901d-41dce353a007"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-5.20417043e-18])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# 사이킷 런에서 제공하는 GBRT 앙상블을 다음과 같이 간단하게 훈련시킬 수 있다.\n","\n","from sklearn.ensemble import GradientBoostingRegressor\n","# GBRT 모형 구축\n","# GBRT 앙상블 모형도 마찬가지로 n_estimator, max_deptj, min_samples_leaf 등을 통해 모델 규제가 가능하다\n","# 추가적으로 learning_rate가 각 트리의 기여 정도를 조절하다.\n","# learning_rate가 0.1보다 낮게 설정되면 훈련을 위한 트리가 더 많이 필요하지만 성능은 좋아진다.\n","# 이러한 방식을 축소(shrinkage)라고 부르는 규제 방법이다.\n","gbrt = GradientBoostingRegressor(max_depth = 3 ,\n","                                 n_estimators = 3,\n","                                 learning_rate = 1)\n","# GBRT 모형 학습\n","gbrt.fit(x, y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2HN9mLDFieUU","executionInfo":{"status":"ok","timestamp":1640704566470,"user_tz":-540,"elapsed":11,"user":{"displayName":"조이준","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18430695341039920489"}},"outputId":"cfba718a-f4cb-4b85-d7e8-9993c5c81a65"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GradientBoostingRegressor(learning_rate=1, n_estimators=3)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":[""],"metadata":{"id":"SbwYUezfnOGL"},"execution_count":null,"outputs":[]}]}